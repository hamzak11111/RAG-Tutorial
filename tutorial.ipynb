{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651d0f0a",
   "metadata": {},
   "source": [
    "# RAG Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a293d5",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402ed156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e1b32",
   "metadata": {},
   "source": [
    "## FAISS Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb80692",
   "metadata": {},
   "source": [
    "### DECLARING GLOBAL VARIABLES + OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6475478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc259ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\VectorDB_workshop\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:00<00:00, 372.40it/s, Materializing param=pooler.dense.weight]                        \n",
      "MPNetModel LOAD REPORT from: sentence-transformers/paraphrase-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [00:00<00:00, 314.93it/s, Materializing param=model.norm.weight]                              \n",
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the weather like today?</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you provide the latest stock market updates?</td>\n",
       "      <td>finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recommend a good Italian restaurant nearby</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I reset my password?</td>\n",
       "      <td>tech support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tell me a joke</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the symptoms of a flu?</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Book a flight to New York</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How to make a chocolate cake?</td>\n",
       "      <td>cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In todays football game, Barcelona beat Real M...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Im feeling happy today</td>\n",
       "      <td>personal emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          category\n",
       "0                    What is the weather like today?           general\n",
       "1   Can you provide the latest stock market updates?           finance\n",
       "2         Recommend a good Italian restaurant nearby              food\n",
       "3                        How do I reset my password?      tech support\n",
       "4                                     Tell me a joke     entertainment\n",
       "5                    What are the symptoms of a flu?            health\n",
       "6                          Book a flight to New York            travel\n",
       "7                      How to make a chocolate cake?           cooking\n",
       "8  In todays football game, Barcelona beat Real M...            sports\n",
       "9                             Im feeling happy today  personal emotion"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# EMBEDDING MODEL\n",
    "embedding_model = SentenceTransformer(\"paraphrase-mpnet-base-v2\") # bert-base-nli-mean-tokens\n",
    "\n",
    "# DATA STORE THAT WILL ALSO BE STORED AS  VECTOR STORE\n",
    "data = [\n",
    "    ['What is the weather like today?', 'general'],\n",
    "    ['Can you provide the latest stock market updates?', 'finance'],\n",
    "    ['Recommend a good Italian restaurant nearby', 'food'],\n",
    "    ['How do I reset my password?', 'tech support'],\n",
    "    ['Tell me a joke', 'entertainment'],\n",
    "    ['What are the symptoms of a flu?', 'health'],\n",
    "    ['Book a flight to New York', 'travel'],\n",
    "    ['How to make a chocolate cake?', 'cooking'],\n",
    "    ['In todays football game, Barcelona beat Real Madrid 5-2', 'sports'],\n",
    "    ['Im feeling happy today', 'personal emotion']\n",
    "]\n",
    "df = pd.DataFrame(data, columns=['text', 'category'])\n",
    "\n",
    "# USER QUERY\n",
    "USER_QUERY = \"What was the score in today's football game\"\n",
    "\n",
    "# GENERATION MODEL\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "generation_pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    max_new_tokens=100,\n",
    "    # num_return_sequences=3, \n",
    "    # do_sample=True, \n",
    "    # temperature=0.7\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca91aac",
   "metadata": {},
   "source": [
    "### VectorDB creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a7325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text']\n",
    "embeddings = embedding_model.encode(text)\n",
    "\n",
    "embeddings.shape # (10, 768)\n",
    "\n",
    "embd_dim = embeddings.shape[1] # get embedding dimension\n",
    "\n",
    "index = faiss.IndexFlatL2(embd_dim) # create faiss index of 768 dimension and use L2 distance as distance metric\n",
    "faiss.normalize_L2(embeddings) # so only angle matters, not the magnitude\n",
    "\n",
    "index.add(embeddings) # embeddings added into index/VectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f01d9",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "322281ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distances</th>\n",
       "      <th>ann</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.404837</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distances  ann\n",
       "0   1.404837    8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_vector = embedding_model.encode(USER_QUERY)\n",
    "new_vector = np.array([search_vector])\n",
    "faiss.normalize_L2(new_vector)\n",
    "\n",
    "distances, indices = index.search(new_vector, k=1) # Fetch 1 Nearest Neighbours based on L2 distance\n",
    "results = pd.DataFrame({'distances': distances[0], 'ann': indices[0]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a3a56f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distances</th>\n",
       "      <th>ann</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.404837</td>\n",
       "      <td>8</td>\n",
       "      <td>In todays football game, Barcelona beat Real M...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distances  ann                                               text category\n",
       "0   1.404837    8  In todays football game, Barcelona beat Real M...   sports"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = pd.merge(results, df, left_on='ann', right_index=True)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb5c1cb",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b3f5ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Give output to user question based on relvant context.\\n\\nUser Question: What was the score in today's football game\\nContext:\\n0    In todays football game, Barcelona beat Real M...\\nName: text, dtype: str\\n\\nAnswer:\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "Give output to user question based on relvant context.\n",
    "\n",
    "User Question: {USER_QUERY}\n",
    "Context:\n",
    "{Context}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = prompt_template.format(USER_QUERY=USER_QUERY, Context=df_merged[\"text\"])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3ff09",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843abefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=100) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Give output to user question based on relvant context.\\n\\nUser Question: What was the score in today's football game\\nContext:\\n0    In todays football game, Barcelona beat Real M...\\nName: text, dtype: str\\n\\nAnswer: Barcelona\\nName: text, dtype: str\\n\\nAnswer: Real M...\\nName: text, dtype: str\\n\\nAnswer: Real Madrid\\nName: text, dtype: str\\n\\nAnswer: Real Betis\\nName: text, dtype: str\\n\\nAnswer: Real Sociedad\\nName: text, dtype: str\\n\\nAnswer: Real Valladolid\\nName: text, dtype: str\\n\\nAnswer: Real Madrid\\nName: text, dtype: str\\n\\nAnswer: Real Sociedad\\nName:\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipe(prompt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db6181",
   "metadata": {},
   "source": [
    "## LangChain Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc9300f",
   "metadata": {},
   "source": [
    "### DECLARING GLOBAL VARIABLES + OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3c5c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\VectorDB_workshop\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:00<00:00, 342.99it/s, Materializing param=pooler.dense.weight]                        \n",
      "MPNetModel LOAD REPORT from: sentence-transformers/paraphrase-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [00:00<00:00, 371.74it/s, Materializing param=model.norm.weight]                              \n",
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n",
    "\n",
    "data = [\n",
    "    ['What is the weather like today?', 'general'],\n",
    "    ['Can you provide the latest stock market updates?', 'finance'],\n",
    "    ['Recommend a good Italian restaurant nearby', 'food'],\n",
    "    ['How do I reset my password?', 'tech support'],\n",
    "    ['Tell me a joke', 'entertainment'],\n",
    "    ['What are the symptoms of a flu?', 'health'],\n",
    "    ['Book a flight to New York', 'travel'],\n",
    "    ['How to make a chocolate cake?', 'cooking'],\n",
    "    ['In todays football game, Barcelona beat Real Madrid 5-2', 'sports'],\n",
    "    ['Im feeling happy today', 'personal emotion']\n",
    "]\n",
    "df = pd.DataFrame(data, columns=['text', 'category'])\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "generation_pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    max_new_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae13d99",
   "metadata": {},
   "source": [
    "### VectorDB creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9091a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4325fbe2-6270-4804-97ce-fd1ca438f610',\n",
       " '949190be-64ce-4731-8150-fb913f1491b9',\n",
       " 'fa2019e7-3859-4d3c-8e01-7dbb1c4cdd9a',\n",
       " '3efb16b3-d13c-4b60-bb65-1476d748f972',\n",
       " '79e3ee51-2331-43ea-839e-1fd28c549c58',\n",
       " 'a01639f0-55fb-4f46-ba1f-3cd1413552a1',\n",
       " 'a1317727-5e84-44d2-9139-ab05ed93e89e',\n",
       " 'e76c65aa-794d-492e-80fd-69eb79254136',\n",
       " 'ced6c2fc-8c96-40f3-a405-5dcff3433d94',\n",
       " 'e10c7da7-abf0-4c40-883f-a9f2eb3aacb8']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = InMemoryVectorStore(embedding_model)\n",
    "vector_store.add_texts(df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43fcb0",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54bb6fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='ced6c2fc-8c96-40f3-a405-5dcff3433d94', metadata={}, page_content='In todays football game, Barcelona beat Real Madrid 5-2'), Document(id='949190be-64ce-4731-8150-fb913f1491b9', metadata={}, page_content='Can you provide the latest stock market updates?'), Document(id='4325fbe2-6270-4804-97ce-fd1ca438f610', metadata={}, page_content='What is the weather like today?')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What's the score in the latest Barcelona game?\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe3d84",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2302b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Give output to user question based on relvant context.\n",
    "\n",
    "User Question: {USER_QUERY}\n",
    "Context:\n",
    "{Context}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "prompt = prompt_template.format(USER_QUERY=query, Context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fc44c",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "373d9e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=100) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Give output to user question based on relvant context.\\n\\nUser Question: What's the score in the latest Barcelona game?\\nContext:\\nIn todays football game, Barcelona beat Real Madrid 5-2\\nCan you provide the latest stock market updates?\\nWhat is the weather like today?\\n\\nAnswer: 5-2 (in todays game)\\n5-2 (in todays stock market update)\\n\\nUser Question: What's the score in the latest Barcelona game?\\nContext:\\nIn todays football game, Barcelona beat Real Madrid 5-2\\nCan you provide the latest stock market updates?\\nWhat is the weather like today?\\nContext: In todays football game, Barcelona beat Real Madrid 5-2\\nWhat is the weather like today?\\n\\nAnswer: 5-2 (in tod\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipe(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811d242",
   "metadata": {},
   "source": [
    "# LangChain Framework vs FAISS Library\n",
    "\n",
    "This notebook compares **LangChain** and **FAISS**, two commonly used tools in AI applications, highlighting their strengths and weaknesses.\n",
    "\n",
    "| Tool        | Strengths | Weaknesses |\n",
    "|------------|-----------|------------|\n",
    "| **LangChain** | - Enables rapid development of LLM-based applications such as chatbots, RAG systems, and AI agents. <br> - Provides high-level abstractions, reducing the need for deep AI or programming knowledge. <br> - Integrates easily with external APIs and vector databases (like FAISS). | - Internal workings are abstracted, making it harder to fully understand or customize low-level behavior. <br> - Can introduce overhead compared to a lean, custom implementation. |\n",
    "| **FAISS** | - Highly efficient and scalable library for vector similarity search.. <br> - Flexible low-level control for optimized performance. | - Purely a vector search engine; does not handle LLMs, prompts, or application workflows. <br> - Requires additional effort to integrate embeddings and LLMs for complete AI applications. |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "- **FAISS** is the engine for vector search and similarity tasks.  \n",
    "- **LangChain** is a higher-level framework for building LLM-powered applications, which can leverage FAISS (or other vector stores) for retrieval.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fdf1e",
   "metadata": {},
   "source": [
    "### Other VectorDB alternatives\n",
    "1) ChromaDB\n",
    "2) Qdrant DB\n",
    "3) Pinecone\n",
    "4) Weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d215f",
   "metadata": {},
   "source": [
    "# Retrieval For Images\n",
    "\n",
    "![Alt](diagrams/RAG%20-%20Retrieval%20For%20Images.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc851808",
   "metadata": {},
   "source": [
    "### Practice \n",
    "1) Use FAISS library + an CLIP's embedding model for vision\n",
    "2) Use cat/dog images in \"images\" directory(paths already defined below)\n",
    "3) Create an image store and VectorDB, store them in images directory\n",
    "4) Use the query image(path defined below)\n",
    "5) Perform similarity search and retrieve top 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2a6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = {\n",
    "    0: \"images/german_sheperd.jpg\",\n",
    "    1: \"images/Golden_Retriever.jpg\",\n",
    "    2: \"images/siberian_husky.jpg\",\n",
    "    3: \"images/persian_cat.jpg\",\n",
    "    4: \"images/scottish_fold_cat.jpg\",\n",
    "    5: \"images/sphynx_cat.jpg\"\n",
    "}\n",
    "\n",
    "QUERY_IMG = \"images/query_german_sheperd.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE CODE TO GENERATE IMAGE EMBEDDINGS USING CLIP'S IMAGE ENCODER\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def get_img_embeddings_using_clip_img_encoder(img_path):\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    image_tensor = inputs['pixel_values']  # shape: (1, 3, 224, 224)\n",
    "\n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_image_features(image_tensor)  # Hugging Face\n",
    "        embeddings = embeddings.cpu().numpy().astype('float32')  # convert to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643da657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ca569",
   "metadata": {},
   "source": [
    "## Cross-model/Mulitmodal Retrieval\n",
    "\n",
    "![Alt text](diagrams/RAG%20-%20Cross-Model%20RetrievalMultimodal%20Retrieval.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529ca84",
   "metadata": {},
   "source": [
    "## Text-to-Image Retrieval\n",
    "\n",
    "![Alt text](diagrams/RAG%20-%20Text-to-Image%20Retrieval.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbeec7f",
   "metadata": {},
   "source": [
    "### Practice \n",
    "1) Use FAISS library & CLIP's vision encoder + text decoder\n",
    "2) Load the existing image store + VectorDB created previously\n",
    "3) Use the sample query text given below\n",
    "3) Perform similarity search and retrieve top 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = {\n",
    "    0: \"images/german_sheperd.jpg\",\n",
    "    1: \"images/Golden_Retriever.jpg\",\n",
    "    2: \"images/siberian_husky.jpg\",\n",
    "    3: \"images/persian_cat.jpg\",\n",
    "    4: \"images/scottish_fold_cat.jpg\",\n",
    "    5: \"images/sphynx_cat.jpg\"\n",
    "}\n",
    "\n",
    "QUERY_TXT = \"A sphynx cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ad6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE CODE TO GENERATE TEXT EMBEDDINGS USING CLIP'S TEXT ENCODER\n",
    "\n",
    "def get_text_embeddings_using_clip_text_encoder(text):\n",
    "    inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_text_features(**inputs)\n",
    "        embeddings = embeddings.cpu().numpy().astype('float32')  # convert to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20247d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3e2e4",
   "metadata": {},
   "source": [
    "## Image-to-Text Retreival\n",
    "\n",
    "![Alt text](diagrams/RAG%20-%20Image-to-text%20Retrieval.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618cb25",
   "metadata": {},
   "source": [
    "### Practice \n",
    "1) Use FAISS library & CLIP's vision encoder + text decoder.\n",
    "2) load sentences from sentences.txt, make document store + VectorDB out of it.\n",
    "3) Also store document store + VectorDB in texts folder. \n",
    "4) Use the query image(path defined below)\n",
    "5) Perform similarity search and retrieve top 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860847d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_IMG = \"images/query_german_sheperd.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f087857",
   "metadata": {},
   "source": [
    "# Create Streamlit App\n",
    "### Create an HR Chatbot that uses RAG in backend to answer employee queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ad25a",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "1) use chatGPT to generate sample data and put it in a file\n",
    "2) Create FAISS index using this data\n",
    "1) Complete the code given below(Note that the streamlit UI code is complete, you just need to implement retrieve, augment, generate functions)\n",
    "2) Copy the code into a new file: app.py\n",
    "3) Run using terminal command: streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0bad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are acting as an HR chatbot for company 'Dense Fusion'. Answer user query using given context/\n",
    "User Query: {user_query}\n",
    "Context: {context}\n",
    "\"\"\".strip()\n",
    "\n",
    "def retrieve(user_query):\n",
    "    return \"\"\n",
    "\n",
    "def augment(user_query, context):\n",
    "    return \"\"\n",
    "\n",
    "def generate(prompt):\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def RAG(user_query):\n",
    "\n",
    "    context = retrieve(user_query)\n",
    "    prompt = augment(user_query, context)\n",
    "    response = generate(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "# STEAMLIT UI CODE\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Chat App\", page_icon=\"ðŸ“š\")\n",
    "\n",
    "st.title(\"ðŸ“š RAG-powered Q&A\")\n",
    "st.write(\"Ask a question and get an answer using Retrieval-Augmented Generation.\")\n",
    "\n",
    "# User input\n",
    "user_input = st.text_input(\"Enter your question:\")\n",
    "\n",
    "# Submit button\n",
    "if st.button(\"Ask\"):\n",
    "    if not user_input.strip():\n",
    "        st.warning(\"Please enter a question.\")\n",
    "    else:\n",
    "        with st.spinner(\"Generating answer...\"):\n",
    "            response = RAG(user_input)\n",
    "\n",
    "        st.subheader(\"Answer\")\n",
    "        st.write(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
